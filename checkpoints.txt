polynomial regression check correctness
generalised linear models  :- poisson, gamma, tweedie
robust regression models :- huber and others
passive aggresive models
LARS
lassolars
TimeSeriesSplit
powertransformer
quantiletransformer
kbinsdiscretizer
functiontransformer
ExtraTreesRegressor
ExtraTreesClassifier

`n_jobs` is a common parameter in many machine learning libraries, such as scikit-learn. It is an integer that specifies the maximum number of concurrently running workers. If `1` is given, no joblib parallelism is used at all, which is useful for debugging. If set to `-1`, all CPUs are used. For `n_jobs` below `-1`, `(n_cpus + 1 + n_jobs)` are used. For example, with `n_jobs=-2`, all CPUs but one are used¹.


In scikit-learn, the `verbose` parameter is an integer that controls the verbosity of the output. The higher the value of `verbose`, the more detailed the output will be. For example, in the case of building a random forest, setting `verbose` to a higher number (e.g., 2 instead of 1) will show more information about the tree building process¹.


`cross_val_score` and `cross_validate` are both cross-validation functions in scikit-learn. They both estimate the generalization performance of a model by splitting the data into multiple parts, training the model on some parts and evaluating it on others.

The main difference between the two functions is that `cross_validate` allows specifying multiple metrics for evaluation, while `cross_val_score` only allows a single metric. Additionally, `cross_validate` returns a dictionary containing not only the test scores but also the training scores, fit times, and score times¹.

In summary, if you need to evaluate your model using multiple metrics or want to access additional information such as fit times and score times, you should use `cross_validate`. Otherwise, `cross_val_score` may suffice².


**Stratified K-Fold** is a variation of K-Fold cross-validation that returns stratified folds. It provides train/test indices to split data into train/test sets. The folds are made by preserving the percentage of samples for each class¹.

In other words, Stratified K-Fold ensures that each fold has the same proportion of samples from each class as the whole dataset. This is particularly useful when dealing with imbalanced datasets, where one class has many more samples than the other².


**K-Fold** cross-validation is a technique for evaluating the performance of a model by splitting the data into `K` parts, training the model on `K-1` parts and evaluating it on the remaining part. This process is repeated `K` times, with each part serving as the test set once.

**Group K-Fold** is a variation of K-Fold that ensures that the same group is not represented in both the training and test sets. This is useful when you have data that is grouped in some way, and you want to ensure that the model is evaluated on groups that it has not seen during training¹.

In summary, you should use Group K-Fold when you have grouped data and want to ensure that the model is evaluated on unseen groups. Otherwise, K-Fold may suffice.

Unlike KFold, GroupKFold is not randomized at all, whereas KFold is randomized when shuffle=True

KFold simply splits the data into k folds of equal size, without considering any grouping of the data. This means that it is possible for the same group to appear in both the training and test sets, which can bias the results of the evaluation.

GroupKFold takes into account any grouping of the data and ensures that the same group does not appear in both the training and test sets. This is useful when the data is grouped in a way that is meaningful to the model, such as by time, location, or user.

In general, GroupKFold is a more robust cross-validation method than KFold. However, it is also more computationally expensive, as it requires more data to be stored.

Repeated K-Fold is a cross-validation technique used in machine learning. It repeats K-Fold cross-validation `n` times with different randomization in each repetition ¹. This technique is used to improve the estimated performance of a machine learning model ². Is there anything specific you would like to know about Repeated K-Fold?

Repeated K-Fold cross-validation improves the estimated performance of a machine learning model by repeating the cross-validation procedure multiple times and reporting the mean result across all folds from all runs ¹. This mean result is expected to be a more accurate estimate of the true unknown underlying mean performance of the model on the dataset, as calculated using the standard error ¹. Repeated K-Fold cross-validation has the benefit of improving the estimate of the mean model performance at the cost of fitting and evaluating many more models ¹. Is there anything else you would like to know?
